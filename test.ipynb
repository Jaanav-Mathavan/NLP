{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating inverted index for P1 Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR preprocessing method\n",
      " \n",
      "eat: ['d3']\n",
      "herbivor: ['d1']\n",
      "meat: ['d1', 'd2']\n",
      "leav: ['d3']\n",
      "eater: ['d1', 'd2']\n",
      "plant: ['d1', 'd2']\n",
      "grass: ['d3']\n",
      "carnivor: ['d2']\n",
      "typic: ['d1', 'd2']\n",
      "deer: ['d3']\n",
      " \n",
      "Question specific method\n",
      " \n",
      "eat: ['d3']\n",
      "typically: ['d1', 'd2']\n",
      "meat: ['d1', 'd2']\n",
      "eaters: ['d1', 'd2']\n",
      "plant: ['d1', 'd2']\n",
      "grass: ['d3']\n",
      "Herbivores: ['d1']\n",
      "Deers: ['d3']\n",
      "Carnivores: ['d2']\n",
      "leaves: ['d3']\n"
     ]
    }
   ],
   "source": [
    "from sentenceSegmentation import SentenceSegmentation\n",
    "from tokenization import Tokenization\n",
    "from inflectionReduction import InflectionReduction\n",
    "\n",
    "docs = {\"d1\": \"Herbivores are typically plant eaters and not meat eaters\",\n",
    "\"d2\": \"Carnivores are typically meat eaters and not plant eaters\",\n",
    "\"d3\": \"Deers eat grass and leaves\"}\n",
    "\n",
    "#Introducing stopwords\n",
    "stopwords = set([\"are\", \"and\", \"not\"])\n",
    "\n",
    "segmenter = SentenceSegmentation()\n",
    "tokenizer = Tokenization()\n",
    "inflectionReducer = InflectionReduction()\n",
    "\n",
    "print(\"IR preprocessing method\")\n",
    "print(\" \")\n",
    "for key in docs:\n",
    "    docs[key] = segmenter.punkt(docs[key])\n",
    "    docs[key] = tokenizer.pennTreeBank(docs[key])\n",
    "    docs[key] = inflectionReducer.reduce(docs[key])\n",
    "    temp_list = []\n",
    "    for segment in docs[key]:\n",
    "        temp_list.append([word for segment in docs[key] for word in segment if word not in stopwords])\n",
    "    docs[key] = temp_list\n",
    "\n",
    "#Deriving unique words \n",
    "words = list(set([word for doc in docs.values() for segment in doc for word in segment]))\n",
    "\n",
    "inverted_index_ir = {}\n",
    "\n",
    "for word in words:\n",
    "    inverted_index_ir[word] = [doc for doc in docs for segment in docs[doc] if word in segment]\n",
    "\n",
    "for key, value in inverted_index_ir.items():\n",
    "    print(f\"{key}: {value}\")    \n",
    "    \n",
    "print(\" \")\n",
    "print(\"Question specific method\")\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "docs = {\"d1\": \"Herbivores are typically plant eaters and not meat eaters\",\n",
    "\"d2\": \"Carnivores are typically meat eaters and not plant eaters\",\n",
    "\"d3\": \"Deers eat grass and leaves\"}\n",
    "\n",
    "inverted_index_qs = {}\n",
    "words = set([word for word in \" \".join(docs.values()).split()])\n",
    "words = list(words - stopwords)\n",
    "for word in words:\n",
    "    inverted_index_qs[word] = [doc for doc in docs if word in docs[doc].split()]\n",
    "    \n",
    "for key, value in inverted_index_qs.items():\n",
    "    print(f\"{key}: {value}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P1 Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR preprocessing method\n",
      " \n",
      "eat: ['d3']\n",
      "herbivor: ['d1']\n",
      "meat: ['d1', 'd2']\n",
      "leav: ['d3']\n",
      "eater: ['d1', 'd2']\n",
      "plant: ['d1', 'd2']\n",
      "grass: ['d3']\n",
      "carnivor: ['d2']\n",
      "typic: ['d1', 'd2']\n",
      "deer: ['d3']\n",
      " \n",
      "TF-IDF Matrix:\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>herbivor</th>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meat</th>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leav</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eater</th>\n",
       "      <td>0.135155</td>\n",
       "      <td>0.135155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant</th>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grass</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carnivor</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typic</th>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deer</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                d1        d2        d3\n",
       "eat       0.000000  0.000000  0.274653\n",
       "herbivor  0.183102  0.000000  0.000000\n",
       "meat      0.067578  0.067578  0.000000\n",
       "leav      0.000000  0.000000  0.274653\n",
       "eater     0.135155  0.135155  0.000000\n",
       "plant     0.067578  0.067578  0.000000\n",
       "grass     0.000000  0.000000  0.274653\n",
       "carnivor  0.000000  0.183102  0.000000\n",
       "typic     0.067578  0.067578  0.000000\n",
       "deer      0.000000  0.000000  0.274653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentenceSegmentation import SentenceSegmentation\n",
    "from tokenization import Tokenization\n",
    "from inflectionReduction import InflectionReduction\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "docs = {\"d1\": \"Herbivores are typically plant eaters and not meat eaters\",\n",
    "\"d2\": \"Carnivores are typically meat eaters and not plant eaters\",\n",
    "\"d3\": \"Deers eat grass and leaves\"}\n",
    "\n",
    "#Introducing stopwords\n",
    "stopwords = set([\"are\", \"and\", \"not\"])\n",
    "\n",
    "segmenter = SentenceSegmentation()\n",
    "tokenizer = Tokenization()\n",
    "inflectionReducer = InflectionReduction()\n",
    "\n",
    "print(\"IR preprocessing method\")\n",
    "print(\" \")\n",
    "for key in docs:\n",
    "    docs[key] = segmenter.punkt(docs[key])\n",
    "    docs[key] = tokenizer.pennTreeBank(docs[key])\n",
    "    docs[key] = inflectionReducer.reduce(docs[key])\n",
    "    temp_list = []\n",
    "    for segment in docs[key]:\n",
    "        temp_list.append([word for segment in docs[key] for word in segment if word not in stopwords])\n",
    "    docs[key] = temp_list\n",
    "\n",
    "#Deriving unique words \n",
    "words = list(set([word for doc in docs.values() for segment in doc for word in segment]))\n",
    "\n",
    "inverted_index_ir = {}\n",
    "\n",
    "for word in words:\n",
    "    inverted_index_ir[word] = [doc for doc in docs for segment in docs[doc] if word in segment]\n",
    "\n",
    "for key, value in inverted_index_ir.items():\n",
    "    print(f\"{key}: {value}\") \n",
    "    \n",
    "#Let the rows of the term-document matrix be the unique words and the columns be the documents. \n",
    "tf_idf_matrix = [[0]*len(docs) for _ in range(len(words))] \n",
    "\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(docs)):\n",
    "        doc_words = [word for segment in list(docs.values())[j] for word in segment]\n",
    "        word_count = len(list(filter(lambda x: x == words[i], doc_words)))\n",
    "        tf = word_count / len(doc_words) if len(doc_words) > 0 else 0\n",
    "        idf = np.log(len(docs) / len(inverted_index_ir[words[i]]))\n",
    "        tf_idf_matrix[i][j] = tf * idf\n",
    "\n",
    "print(\" \")\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(\" \")\n",
    "data = pd.DataFrame(tf_idf_matrix, index=words, columns=docs.keys()) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P1 Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR preprocessing method\n",
      " \n",
      "eat: ['d3']\n",
      "herbivor: ['d1']\n",
      "meat: ['d1', 'd2']\n",
      "leav: ['d3']\n",
      "eater: ['d1', 'd2']\n",
      "plant: ['d1', 'd2']\n",
      "grass: ['d3']\n",
      "carnivor: ['d2']\n",
      "typic: ['d1', 'd2']\n",
      "deer: ['d3']\n",
      " \n",
      "TF-IDF Matrix:\n",
      " \n",
      "[[0.         0.         0.27465307]\n",
      " [0.18310205 0.         0.        ]\n",
      " [0.06757752 0.06757752 0.        ]\n",
      " [0.         0.         0.27465307]\n",
      " [0.13515504 0.13515504 0.        ]\n",
      " [0.06757752 0.06757752 0.        ]\n",
      " [0.         0.         0.27465307]\n",
      " [0.         0.18310205 0.        ]\n",
      " [0.06757752 0.06757752 0.        ]\n",
      " [0.         0.         0.27465307]]\n",
      " \n",
      "                d1        d2        d3\n",
      "eat       0.000000  0.000000  0.274653\n",
      "herbivor  0.183102  0.000000  0.000000\n",
      "meat      0.067578  0.067578  0.000000\n",
      "leav      0.000000  0.000000  0.274653\n",
      "eater     0.135155  0.135155  0.000000\n",
      "plant     0.067578  0.067578  0.000000\n",
      "grass     0.000000  0.000000  0.274653\n",
      "carnivor  0.000000  0.183102  0.000000\n",
      "typic     0.067578  0.067578  0.000000\n",
      "deer      0.000000  0.000000  0.274653\n",
      " \n",
      "Query: plant eaters\n",
      " \n",
      "Cosine Similarity: \n",
      " \n",
      "[0.0, 0.0, 0.0, 0.0, 0.5493061443340549, 0.5493061443340549, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.56015692 0.56015692 0.        ]\n",
      "Ranked documents:\n",
      " \n",
      "Document d2: 0.560156917515788\n",
      "Document d1: 0.560156917515788\n",
      "Document d3: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sentenceSegmentation import SentenceSegmentation\n",
    "from tokenization import Tokenization\n",
    "from inflectionReduction import InflectionReduction\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "docs = {\"d1\": \"Herbivores are typically plant eaters and not meat eaters\",\n",
    "\"d2\": \"Carnivores are typically meat eaters and not plant eaters\",\n",
    "\"d3\": \"Deers eat grass and leaves\"}\n",
    "\n",
    "#Introducing stopwords\n",
    "stopwords = set([\"are\", \"and\", \"not\"])\n",
    "\n",
    "segmenter = SentenceSegmentation()\n",
    "tokenizer = Tokenization()\n",
    "inflectionReducer = InflectionReduction()\n",
    "\n",
    "print(\"IR preprocessing method\")\n",
    "print(\" \")\n",
    "for key in docs:\n",
    "    docs[key] = segmenter.punkt(docs[key])\n",
    "    docs[key] = tokenizer.pennTreeBank(docs[key])\n",
    "    docs[key] = inflectionReducer.reduce(docs[key])\n",
    "    temp_list = []\n",
    "    for segment in docs[key]:\n",
    "        temp_list.append([word for segment in docs[key] for word in segment if word not in stopwords])\n",
    "    docs[key] = temp_list\n",
    "\n",
    "#Deriving unique words \n",
    "words = list(set([word for doc in docs.values() for segment in doc for word in segment]))\n",
    "\n",
    "inverted_index_ir = {}\n",
    "\n",
    "for word in words:\n",
    "    inverted_index_ir[word] = [doc for doc in docs for segment in docs[doc] if word in segment]\n",
    "\n",
    "for key, value in inverted_index_ir.items():\n",
    "    print(f\"{key}: {value}\") \n",
    "    \n",
    "#Let the rows of the term-document matrix be the unique words and the columns be the documents. \n",
    "tf_idf_matrix = [[0]*len(docs) for _ in range(len(words))] \n",
    "idf_list = []\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(docs)):\n",
    "        doc_words = [word for segment in list(docs.values())[j] for word in segment]\n",
    "        word_count = len(list(filter(lambda x: x == words[i], doc_words)))\n",
    "        tf = word_count / len(doc_words) if len(doc_words) > 0 else 0\n",
    "        idf = np.log(len(docs) / len(inverted_index_ir[words[i]]))\n",
    "        tf_idf_matrix[i][j] = tf * idf\n",
    "        idf_list.append(idf)\n",
    "\n",
    "print(\" \")\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(\" \")\n",
    "data = pd.DataFrame(tf_idf_matrix, index=words, columns=docs.keys()) \n",
    "print(np.array(tf_idf_matrix))\n",
    "print(\" \")\n",
    "print(data)\n",
    "print(\" \")\n",
    "query = \"plant eaters\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\" \")\n",
    "\n",
    "def cosine_similarity_matrix(doc_matrix, query_vector):\n",
    "    doc_norms = np.linalg.norm(doc_matrix, axis=0)\n",
    "    query_norm = np.linalg.norm(query_vector) \n",
    "    #Avoid division by zero\n",
    "    doc_norms[doc_norms == 0] = 1e-10\n",
    "    query_norm = query_norm if query_norm != 0 else 1e-10\n",
    "    similarities = doc_matrix.T @ query_vector / (doc_norms * query_norm)\n",
    "    return similarities\n",
    "\n",
    "print(\"Cosine Similarity: \")\n",
    "print(\" \")\n",
    "query = segmenter.punkt(query)\n",
    "query = tokenizer.pennTreeBank(query)\n",
    "query = inflectionReducer.reduce(query)\n",
    "temp_list = []\n",
    "for segment in query:\n",
    "    temp_list.append([word for segment in query for word in segment if word not in stopwords])\n",
    "query = temp_list\n",
    "tf_idf_query = []\n",
    "for p in range(len(words)):\n",
    "    query_words = []\n",
    "    for sentence in query: \n",
    "        for word in sentence:\n",
    "            query_words.append(word)\n",
    "    word_count = len(list(filter(lambda x: x == words[p], query_words)))\n",
    "    tf = word_count / len(query_words) if len(query_words) > 0 else 0\n",
    "    tf_idf_query.append(tf * idf_list[p])\n",
    "print(tf_idf_query)\n",
    "\n",
    "cosine_similarities = cosine_similarity_matrix(np.array(tf_idf_matrix), tf_idf_query) \n",
    "print(cosine_similarities)\n",
    "rankings = np.argsort(cosine_similarities)[::-1]\n",
    "print(\"Ranked documents:\")\n",
    "print(\" \")\n",
    "for i in rankings:\n",
    "    print(f\"Document {list(docs.keys())[i]}: {cosine_similarities[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Mr. Johnson and Mrs. Smith arrived at 10 a.m. to discuss the project.\n",
      "He is working for X.Y.Z company.\n",
      "2. Dr. Williams presented the latest findings, e.g., the new experimental results, which showed promising outcomes [Time now is 10 a.m.].\n",
      "They also reviewed various strategies, including market analysis, customer feedback.\n",
      "Despite the lengthy meeting, Mr. and Mrs. Brown remained engaged throughout the discussion.\n"
     ]
    }
   ],
   "source": [
    "text = \"1. Mr. Johnson and Mrs. Smith arrived at 10 a.m. to discuss the project. He is working for X.Y.Z company. 2. Dr. Williams presented the latest findings, e.g., the new experimental results, which showed promising outcomes [Time now is 10 a.m.]. They also reviewed various strategies, including market analysis, customer feedback. Despite the lengthy meeting, Mr. and Mrs. Brown remained engaged throughout the discussion.\"\n",
    "segmentedText = None \n",
    "exceptions = [\"Mr\", \"Mrs\", \"Ms\", \"Dr\", \"Prof\", \"Inc\", \"no\", \"rev\", \"Ltd\" \"e.g\", \"i.e\", \"a.m\", \"p.m\"]\n",
    "exceptions += [str(char) for char in range(9)] \n",
    "punctuations = \".?!;\"\n",
    "segmentedText = []\n",
    "words = text.split(\" \")\n",
    "temp_list = []\n",
    "for word in words:\n",
    "    if word == \"\":\n",
    "        continue\n",
    "    if any(exception in word for exception in exceptions):\n",
    "        if any(char in word for char in \")}]\"):\n",
    "            for bracket in \")}]\":\n",
    "                if bracket in word:\n",
    "                    temp_list.append(word)\n",
    "                    break\n",
    "            if any(char == word[-1] for char in punctuations):\n",
    "                segmentedText.append(\" \".join(temp_list))\n",
    "                temp_list = [] \n",
    "        else:\n",
    "            temp_list.append(word)\n",
    "    else:\n",
    "        if any(char == word[-1] for char in punctuations):\n",
    "            temp_list.append(word)\n",
    "            segmentedText.append(\" \".join(temp_list))\n",
    "            temp_list = [] \n",
    "        else:\n",
    "            temp_list.append(word)  \n",
    "#Fill in code here\n",
    "for t in segmentedText:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Johnson and Mrs. Smith arrived at 10 a.m. to discuss the project.\n",
      "He is working for X.Y.Z Inc. .\n",
      "Wait... Dr. Williams presented the latest findings, e.g., the new experimental results, which showed promising outcomes [Time now is 10 a.m.].\n",
      "They also reviewed various strategies, including market analysis, customer feedback.\n",
      "Despite the lengthy meeting, Mr. and Mrs. Brown remained engaged throughout the discussion.\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "sentence_tokenizer = nltk.tokenize.sent_tokenize\n",
    "text = \"Mr. Johnson and Mrs. Smith arrived at 10 a.m. to discuss the project. He is working for X.Y.Z Inc. . Wait... Dr. Williams presented the latest findings, e.g., the new experimental results, which showed promising outcomes [Time now is 10 a.m.]. They also reviewed various strategies, including market analysis, customer feedback. Despite the lengthy meeting, Mr. and Mrs. Brown remained engaged throughout the discussion.\"\n",
    "result = sentence_tokenizer(text)\n",
    "for res in result:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.\n",
      "Johnson and Mrs.\n",
      "Smith arrived at 10 a.m.\n",
      "to discuss the project.\n",
      "He is working for X.Y.Z Inc.\n",
      ".\n",
      "Wait... Dr.\n",
      "Williams presented the latest findings, e.g., the new experimental results, which showed promising outcomes [Time now is 10 a.m.].\n",
      "They also reviewed various strategies, including market analysis, customer feedback.\n",
      "Despite the lengthy meeting, Mr.\n",
      "and Mrs.\n",
      "Brown remained engaged throughout the discussion.\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "text = \"Mr. Johnson and Mrs. Smith arrived at 10 a.m. to discuss the project. He is working for X.Y.Z Inc. . Wait... Dr. Williams presented the latest findings, e.g., the new experimental results, which showed promising outcomes [Time now is 10 a.m.]. They also reviewed various strategies, including market analysis, customer feedback. Despite the lengthy meeting, Mr. and Mrs. Brown remained engaged throughout the discussion.\"\n",
    "result = sentence_tokenizer.tokenize(text)\n",
    "for res in result:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Despite',\n",
       " 'the',\n",
       " 'lengthy',\n",
       " 'meeting',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'and',\n",
       " 'Mrs.',\n",
       " 'Brown',\n",
       " 'remained',\n",
       " 'engaged',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'discussion',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = \"Despite the lengthy meeting, Mr. and Mrs. Brown remained engaged throughout the discussion.\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " 'Thanks']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = '''Good muffins cost $3.88\\nin New York Please buy me\\ntwo of them\\nThanks'''\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', 'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks.', '``', 'Meet', 'DrX', 'at', '5', 'p.m.', 'in', 'Lab-2.', 'State-of-the-art', 'models', 'are', 'expensive.', '+3.14', 'is', 'pi']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Good muffins cost $3.88 in New York Please buy me two of them Thanks. \"Meet DrX at 5 p.m. in Lab-2. State-of-the-art models are expensive. +3.14 is pi'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
    "s = '''Good muffins cost $3.88\\nin New York  Please buy me\\ntwo of them\\nThanks. \"Meet DrX at 5 p.m. in Lab-2. State-of-the-art models are expensive. +3.14 is pi'''\n",
    "d = TreebankWordDetokenizer()\n",
    "t = TreebankWordTokenizer()\n",
    "toks = t.tokenize(s)\n",
    "print(toks)\n",
    "d.detokenize(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text) == toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", \"John's\", 'car', '.', 'Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '/', 'static', '/', 'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks', '.', 'Mr.', 'and', 'Mrs.', \"Let's\", 'have', 'some', '3.', '+', '3.14', 'is', 'Pi', '.', 'State', '-', 'of', '-', 'the', '-', 'art', 'model']\n"
     ]
    }
   ],
   "source": [
    "segment = '''It's John's car. Good muffins cost $3.88\\nin New York /static/ Please buy me\\ntwo of them\\nThanks. Mr. and Mrs. Let's have some 3. +3.14 is Pi. State-of-the-art model'''\n",
    "punctuations = \".,!?;:'\\\"()[]{}$\"\n",
    "exceptions = [\"Mr\", \"Mrs\", \"Ms\", \"Dr\", \"Prof\", \"Inc\", \"no\", \"rev\", \"Ltd\", \"e.g\", \"i.e\", \"a.m\", \"p.m\"]\n",
    "exceptions += [str(i) for i in range(10)]\n",
    "\n",
    "tokens = []\n",
    "current_word = \"\"\n",
    "\n",
    "for char in segment:\n",
    "    if char.isalnum() or char == \"'\":\n",
    "        current_word += char\n",
    "    else:\n",
    "        if current_word and current_word in exceptions and char == \".\":\n",
    "            current_word += char\n",
    "            continue\n",
    "        if current_word:\n",
    "            tokens.append(current_word)\n",
    "            current_word = \"\"\n",
    "        if char in punctuations or char in \"+-/*^&@#%^\":\n",
    "            tokens.append(char)\n",
    "\n",
    "if current_word:\n",
    "    tokens.append(current_word)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
